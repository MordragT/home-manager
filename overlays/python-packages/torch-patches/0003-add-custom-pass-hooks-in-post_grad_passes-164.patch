From e2c83750f5761cd527941e134ba791b21ee58ad6 Mon Sep 17 00:00:00 2001
From: "Yu, Guangye" <106960996+guangyey@users.noreply.github.com>
Date: Fri, 15 Sep 2023 00:39:22 +0800
Subject: [PATCH 03/17] add custom pass hooks in post_grad_passes (#164)

---
 test/inductor/test_custom_post_grad_passes.py | 172 ++++++++++++++++++
 torch/_inductor/config.py                     |  16 ++
 torch/_inductor/fx_passes/post_grad.py        |   6 +
 3 files changed, 194 insertions(+)
 create mode 100644 test/inductor/test_custom_post_grad_passes.py

diff --git a/test/inductor/test_custom_post_grad_passes.py b/test/inductor/test_custom_post_grad_passes.py
new file mode 100644
index 00000000000..7f8124ccd55
--- /dev/null
+++ b/test/inductor/test_custom_post_grad_passes.py
@@ -0,0 +1,172 @@
+# Owner(s): ["module: inductor"]
+import contextlib
+
+import torch
+import torch._inductor.pattern_matcher as pattern_matcher
+
+from torch._dynamo.test_case import run_tests, TestCase
+from torch._dynamo.utils import counters
+
+from torch._inductor import config
+from torch._inductor.lowering import lowerings as L
+from torch._inductor.pattern_matcher import Arg, CallFunction, PatternMatcherPass
+
+from torch.testing._internal.common_utils import IS_LINUX
+from torch.testing._internal.inductor_utils import HAS_CPU
+
+
+@config.patch({"freezing": True})
+class TestCustomPassBase(TestCase):
+    def _clone_inputs(self, inputs):
+        def clone(x):
+            if not isinstance(x, torch.Tensor):
+                return x
+            return x.clone()
+
+        return tuple(clone(x) for x in inputs)
+
+    def _test_common(
+        self,
+        mod,
+        inputs,
+        matcher_count,
+        matcher_nodes,
+        atol=1e-5,
+        rtol=1.3e-6,
+    ):
+        counters.clear()
+        maybe_autocast = contextlib.nullcontext()
+        with torch.no_grad(), maybe_autocast:
+            clone_inputs = self._clone_inputs(inputs)
+            expected = mod(*inputs)
+            actual = torch.compile(mod)(*clone_inputs)
+            torch.testing.assert_close(actual, expected, atol=atol, rtol=rtol)
+            self.assertEqual(
+                counters["inductor"]["pattern_matcher_count"], matcher_count
+            )
+            self.assertEqual(
+                counters["inductor"]["pattern_matcher_nodes"],
+                matcher_nodes,
+            )
+
+
+aten = torch.ops.aten
+mkldnn = torch.ops.mkldnn
+
+
+class TestPostGradCustomPrePostPass(TestCustomPassBase):
+    #  mkldnn fusion's pattern_matcher
+    # (torch/_inductor/fx_passes/mkldnn_fusion.py),
+    # and apply it to custom post_grad_passes.
+    def _register_mkldnn_conv_relu_fusion(self, custom_pass_dict):
+        # pattern
+        def _mkldnn_conv_relu_pattern():
+            return CallFunction(
+                aten.relu,
+                CallFunction(
+                    mkldnn._convolution_pointwise.default,
+                    Arg(),
+                    Arg(),
+                    Arg(),
+                    Arg(),
+                    Arg(),
+                    Arg(),
+                    Arg(),
+                    Arg(),
+                    Arg(),
+                    Arg(),
+                    _users=1,
+                ),
+            )
+
+        # utils of pattern matcher registration
+        def _register_fusion_lowering(pattern, custom_pass_dict):
+            def dummy_check(m):
+                return True
+
+            def register_custom_lowering_pattern(
+                pattern, extra_check, custom_pass_dict
+            ):
+                return pattern_matcher.register_lowering_pattern(
+                    pattern, extra_check, pass_dict=custom_pass_dict
+                )
+
+            @register_custom_lowering_pattern(pattern, dummy_check, custom_pass_dict)
+            def fn(match, *args, **kwargs):
+                computation_args = list(args)[:-3] + ["relu", [], ""]
+                return L[mkldnn._convolution_pointwise.default](*computation_args)
+
+            return fn
+
+        _register_fusion_lowering(_mkldnn_conv_relu_pattern(), custom_pass_dict)
+
+    # custom post grad pass
+    class _CustomPass(PatternMatcherPass):
+        def __init__(self):
+            super().__init__()
+
+        def __call__(self, g: torch.fx.graph.Graph):
+            self.apply(g)
+
+    # case model
+    class _ConvReLU(torch.nn.Module):
+        def __init__(self, ic, oc):
+            super().__init__()
+            self.conv = torch.nn.Conv2d(ic, oc, kernel_size=3, stride=1, padding=1)
+
+        def forward(self, x):
+            x1 = self.conv(x)
+            return x1.relu()
+
+    def test_custom_pre_pass(self):
+        # leave custom pass only in post_grad_passes()
+        dafault_pattern_matcher = config.pattern_matcher
+        config.pattern_matcher = False
+        # define pattern match as custom post grad opt pass
+        config.post_grad_custom_pre_pass = self._CustomPass()
+        config.post_grad_custom_post_pass = None
+        # init mkldnn fusion on custom_matcher
+        self._register_mkldnn_conv_relu_fusion(config.post_grad_custom_pre_pass)
+
+        mod = self._ConvReLU(16, 16).eval()
+        x = torch.randn((1, 16, 56, 56), dtype=torch.float32)
+
+        match_count = 1
+        match_nodes = 2
+        other_match_count = 1  # conv prepack weight
+        other_match_nodes = 1  # conv prepack weight
+        self._test_common(
+            mod, (x,), match_count + other_match_count, match_nodes + other_match_nodes
+        )
+
+        # restore default pattern_matcher
+        config.pattern_matcher = dafault_pattern_matcher
+
+    def test_custom_post_pass(self):
+        # leave custom pass only in post_grad_passes()
+        dafault_pattern_matcher = config.pattern_matcher
+        config.pattern_matcher = False
+        # define pattern match as custom post grad opt pass
+        config.post_grad_custom_pre_pass = None
+        config.post_grad_custom_post_pass = self._CustomPass()
+        # init mkldnn fusion on custom_matcher
+        self._register_mkldnn_conv_relu_fusion(config.post_grad_custom_post_pass)
+
+        mod = self._ConvReLU(16, 16).eval()
+        x = torch.randn((1, 16, 56, 56), dtype=torch.float32)
+
+        match_count = 1
+        match_nodes = 2
+        other_match_count = 1  # conv prepack weight
+        other_match_nodes = 1  # conv prepack weight
+        self._test_common(
+            mod, (x,), match_count + other_match_count, match_nodes + other_match_nodes
+        )
+
+        # restore default pattern_matcher
+        config.pattern_matcher = dafault_pattern_matcher
+
+
+if __name__ == "__main__":
+    if IS_LINUX and HAS_CPU and torch.backends.mkldnn.is_available():
+        run_tests()
diff --git a/torch/_inductor/config.py b/torch/_inductor/config.py
index 579546851e5..1d711d4047c 100644
--- a/torch/_inductor/config.py
+++ b/torch/_inductor/config.py
@@ -45,6 +45,22 @@ epilogue_fusion_first = False
 # enable pattern match+replace optimizations
 pattern_matcher = True
 
+# register custom graph optimizatin pass hook. so far, pre/post passes are
+# only applied before/after pattern_matcher in post_grad_passes.
+#
+# def my_custom_pre_pass(graph: torch.fx.graph.Graph):
+#     # my custom graph optimization pass
+#     ...
+#
+# def my_custom_post_pass(graph: torch.fx.graph.Graph):
+#     # my custom graph optimization pass
+#     ...
+#
+# torch._inductor.config.post_grad_custom_pre_pass = my_custom_pre_pass
+# torch._inductor.config.post_grad_custom_post_pass = my_custom_post_pass
+post_grad_custom_pre_pass = None
+post_grad_custom_post_pass = None
+
 # Optimize away split cat patterns (Experimental)
 split_cat_fx_passes = True
 
diff --git a/torch/_inductor/fx_passes/post_grad.py b/torch/_inductor/fx_passes/post_grad.py
index 925702f0762..74706bcd5fc 100644
--- a/torch/_inductor/fx_passes/post_grad.py
+++ b/torch/_inductor/fx_passes/post_grad.py
@@ -61,6 +61,9 @@ def post_grad_passes(gm: torch.fx.GraphModule, is_inference: bool):
     if is_inference and config.reordering:
         reorder_for_locality(gm.graph)
 
+    if config.post_grad_custom_pre_pass is not None:
+        config.post_grad_custom_pre_pass(gm.graph)
+
     if config.pattern_matcher:
         lazy_init()
 
@@ -72,6 +75,9 @@ def post_grad_passes(gm: torch.fx.GraphModule, is_inference: bool):
         if is_inference:
             inference_patterns.apply(gm.graph)
 
+    if config.post_grad_custom_post_pass is not None:
+        config.post_grad_custom_post_pass(gm.graph)
+
     stable_topological_sort(gm.graph)
     gm.recompile()
     gm.graph.lint()
-- 
2.34.1

