From b43f1ba7bb099f8e04d1670ada70440b05149e0e Mon Sep 17 00:00:00 2001
From: "Xunsong, Huang" <xunsong.huang@intel.com>
Date: Fri, 27 Oct 2023 21:07:29 +0800
Subject: [PATCH 15/17] [Profiler] [Kineto+Legacy] Enable Profiler with Legacy
 & Kineto Support Together (#179)

* Enable XPU profiling with Kineto support

Signed-off-by: Xunsong, Huang <xunsong.huang@intel.com>

* Re-enable legacy profiler together with Kineto

Signed-off-by: Xunsong, Huang <xunsong.huang@intel.com>

---------

Signed-off-by: Xunsong, Huang <xunsong.huang@intel.com>
---
 torch/autograd/profiler.py          |  77 +++++---
 torch/autograd/profiler_legacy.py   |  68 +++----
 torch/autograd/profiler_util.py     | 265 +++++++++++++++-------------
 torch/csrc/profiler/kineto_shim.cpp |  17 +-
 torch/profiler/profiler.py          |  14 +-
 5 files changed, 251 insertions(+), 190 deletions(-)

diff --git a/torch/autograd/profiler.py b/torch/autograd/profiler.py
index a784b09176f..bced307ea51 100644
--- a/torch/autograd/profiler.py
+++ b/torch/autograd/profiler.py
@@ -128,7 +128,8 @@ class profile:
         use_cuda (bool, optional): Enables timing of CUDA events as well using the cudaEvent API.
             Adds approximately 4us of overhead to each tensor operation.
 
-        use_xpu (bool, optional): Enables timing of XPU events as well using the xpuEvent API.
+        use_xpu (bool, optional): Enables timing of XPU events.
+            Only supports Kineto profiling while XPU backend is available.
 
         record_shapes (bool, optional): If shapes recording is set, information
             about input dimensions will be collected. This allows one to see which
@@ -207,8 +208,8 @@ class profile:
         enabled=True,
         *,
         use_cuda=False,
-        use_device=None,
         use_xpu=False,
+        use_device=None,
         record_shapes=False,
         with_flops=False,
         profile_memory=False,
@@ -223,10 +224,10 @@ class profile:
         if not self.enabled:
             return
         self.use_cuda = use_cuda
+        self.use_xpu = use_xpu
         self.use_device: Optional[str] = (
             use_device if use_device != "privateuseone" else None
         )
-        self.use_xpu = use_xpu
         self.function_events: Optional[EventList] = None
         self.entered = False
         self.record_shapes = record_shapes
@@ -250,10 +251,11 @@ class profile:
         if self.use_device == "cuda":
             self.use_device = None
             self.use_cuda = True
-
-        if self.use_device == "xpu":
+            self.use_xpu = False
+        elif self.use_device == "xpu":
             self.use_device = None
-            self.use_xpu= True
+            self.use_cuda = False
+            self.use_xpu = True
 
         if self.use_device and self.use_device != _get_privateuse1_backend_name():
             warn(f"{self.use_device} doesn't support profile.")
@@ -263,7 +265,8 @@ class profile:
             warn("CUDA is not available, disabling CUDA profiling")
             self.use_cuda = False
 
-        if self.use_xpu and not (hasattr(torch, "xpu") and torch.xpu.is_available()):    # type: ignore[attr-defined]
+        if self.use_xpu and \
+            (not hasattr(torch, 'xpu') or not torch.xpu.is_available()):  # type: ignore[attr-defined]
             warn("XPU is not available, disabling XPU profiling")
             self.use_xpu = False
 
@@ -280,10 +283,13 @@ class profile:
                 self.profiler_kind = ProfilerState.KINETO_GPU_FALLBACK
             else:
                 self.kineto_activities.add(ProfilerActivity.CUDA)
-        elif self.use_xpu:
-            # legacy XPU mode
-            self.profiler_kind = ProfilerState.XPU
-
+        if self.use_xpu:
+            if not use_kineto or ProfilerActivity.XPU not in _supported_activities():
+                assert self.use_cpu, "Legacy XPU profiling requires use_cpu=True"
+                warn("Legacy XPU profiling will be deprecated soon")
+                self.profiler_kind = ProfilerState.XPU
+            else:
+                self.kineto_activities.add(ProfilerActivity.XPU)
         if self.use_device:
             if (
                 not use_kineto
@@ -335,14 +341,16 @@ class profile:
             return
         if self.use_cuda:
             torch.cuda.synchronize()
+        if self.use_xpu:
+            torch.xpu.synchronize()   # type: ignore[attr-defined]
         self.kineto_results = _disable_profiler()
         _run_on_profiler_stop()
         parsed_results = self._parse_kineto_results(self.kineto_results)
         self.function_events = EventList(
             parsed_results,
             use_cuda=self.use_cuda,
-            use_device=self.use_device,
             use_xpu=self.use_xpu,
+            use_device=self.use_device,
             profile_memory=self.profile_memory,
             with_flops=self.with_flops,
         )
@@ -452,6 +460,13 @@ class profile:
                 else 0
             )
 
+        def _xpu_memory_usage(mem_record):
+            return (
+                mem_record.nbytes()
+                if mem_record.device_type() in [DeviceType.XPU]
+                else 0
+            )
+
         def _privateuse1_memory_usage(mem_record):
             return (
                 mem_record.nbytes()
@@ -462,6 +477,7 @@ class profile:
         # Create and return FunctionEvent list
         function_events = []
         device_corr_map: Dict[int, List[FunctionEvent]] = {}
+        device_corr_map_values: List[FunctionEvent] = []
         max_evt_id = 0
         for kineto_event in result.events():
             if _filter_name(kineto_event.name()):
@@ -472,6 +488,7 @@ class profile:
 
             cpu_memory_usage = 0
             cuda_memory_usage = 0
+            xpu_memory_usage = 0
             privateuse1_memory_usage = 0
             if kineto_event.device_type() == DeviceType.CPU:
                 # find the corresponding memory allocation events
@@ -480,6 +497,7 @@ class profile:
                 ):
                     cpu_memory_usage += _cpu_memory_usage(mem_record[0])
                     cuda_memory_usage += _cuda_memory_usage(mem_record[0])
+                    xpu_memory_usage += _xpu_memory_usage(mem_record[0])
                     privateuse1_memory_usage += _privateuse1_memory_usage(mem_record[0])
                     mem_record[1] = True
 
@@ -506,6 +524,7 @@ class profile:
                 use_device=self.use_device,
                 cpu_memory_usage=cpu_memory_usage,
                 cuda_memory_usage=cuda_memory_usage,
+                xpu_memory_usage=xpu_memory_usage,
                 privateuse1_memory_usage=privateuse1_memory_usage,
                 is_async=is_async,
                 sequence_nr=kineto_event.sequence_nr(),
@@ -518,20 +537,13 @@ class profile:
                 if self.use_device:
                     privateuse1_time = kineto_event.privateuse1_elapsed_us()
                     if privateuse1_time > 0:
-                        fe.append_kernel(
-                            fe.name,
-                            torch.device("{}:{}".format(self.use_device, fe.device_index)),
-                            fe.device_index,
-                            privateuse1_time)
+                        fe.append_kernel(fe.name, fe.device_index, "privateuse1", privateuse1_time)
                         fe.is_legacy = True
                 else:
                     # Check if we have CUDA time as a fallback
                     cuda_time = kineto_event.cuda_elapsed_us()
                     if cuda_time > 0:
-                        fe.append_kernel(
-                            fe.name,
-                            torch.device("cuda:{}".format(fe.device_index)),
-                            cuda_time)
+                        fe.append_kernel(fe.name, fe.device_index, "cuda", cuda_time)
                         fe.is_legacy = True
             function_events.append(fe)
             corr_id = kineto_event.linked_correlation_id()
@@ -539,22 +551,29 @@ class profile:
                 if corr_id not in device_corr_map:
                     device_corr_map[corr_id] = []
                 device_corr_map[corr_id].append(fe)
+        # prepare the values list for corr map to avoid duplicately appending kernel
+        # while some CPU Kineto (e.g. CUDA/XPU Runtime ) events have a same
+        # correlation ID as CPU events
+        device_corr_map_values = [fe for fe_list in device_corr_map.values() for fe in fe_list]
 
-        # associate CUDA kernels and CUDA runtime (CPU) with CPU events
+        # associate CUDA/XPU kernels and CUDA/XPU runtime (CPU) with CPU events
         for fe in function_events:
             if (
                 fe.device_type == DeviceType.CPU
                 and not fe.is_async
                 and fe.id in device_corr_map
+                and fe not in device_corr_map_values
             ):
                 for f_evt in device_corr_map[fe.id]:
-                    if f_evt.device_type == DeviceType.CUDA:
+                    if f_evt.device_type in [DeviceType.CUDA, DeviceType.XPU]:
                         fe.append_kernel(
                             f_evt.name,
-                            torch.device("cuda:{}".format(fe.device_index)),
-                            f_evt.time_range.end - f_evt.time_range.start)
+                            f_evt.device_index,
+                            "cuda" if f_evt.device_type == DeviceType.CUDA else "xpu",
+                            f_evt.time_range.end - f_evt.time_range.start,
+                        )
                     elif f_evt.device_type == DeviceType.CPU:
-                        # make sure that 'thread' of a CPU Kineto (e.g. CUDA Runtime) event is associated
+                        # make sure that 'thread' of a CPU Kineto (e.g. CUDA/XPU Runtime) event is associated
                         # with the 'thread' of the corresponding linked PyTorch event to properly track
                         # parents and children
                         f_evt.thread = fe.thread
@@ -575,6 +594,7 @@ class profile:
                 use_device=self.use_device,
                 cpu_memory_usage=_cpu_memory_usage(evt),
                 cuda_memory_usage=_cuda_memory_usage(evt),
+                xpu_memory_usage=_xpu_memory_usage(evt),
                 privateuse1_memory_usage=_privateuse1_memory_usage(evt),
                 is_async=False,
                 sequence_nr=-1,
@@ -990,9 +1010,8 @@ def parse_nvprof_trace(path):
         assert row["cbid"] == 211
         evt = functions_map[row["marker_id"]]
         evt.append_kernel(
-            row["kernel_name"],
-            torch.device("cuda:0"),
-            row["kernel_end"] - row["kernel_start"])
+            row["kernel_name"], 0, "cuda", row["kernel_end"] - row["kernel_start"]
+        )
 
     functions.sort(key=lambda evt: evt.time_range.start)
     return functions
diff --git a/torch/autograd/profiler_legacy.py b/torch/autograd/profiler_legacy.py
index 6ac237cf2b5..7e8d99c0a7e 100644
--- a/torch/autograd/profiler_legacy.py
+++ b/torch/autograd/profiler_legacy.py
@@ -18,7 +18,6 @@ from torch.autograd.profiler_util import (
     EventList,
     FunctionEvent,
     MEMORY_EVENT_NAME,
-    Interval,
 )
 
 __all__ = ["profile"]
@@ -59,7 +58,7 @@ class profile:
             warn("CUDA is not available, disabling CUDA profiling")
             self.use_cuda = False
 
-        if self.use_xpu and not (hasattr(torch, "xpu") and torch.xpu.is_available()):    # type: ignore[attr-defined]
+        if self.use_xpu and not (hasattr(torch, "xpu") and torch.xpu.is_available()):   # type: ignore[attr-defined]
             warn("XPU is not available, disabling XPU profiling")
             self.use_xpu = False
 
@@ -100,7 +99,7 @@ class profile:
         if self.use_cuda:
             torch.cuda.synchronize()
         if self.use_xpu:
-            torch.xpu.synchronize()    # type: ignore[attr-defined]
+            torch.xpu.synchronize()     # type: ignore[attr-defined]
 
         records = _disable_profiler_legacy()
         parsed_results = _parse_legacy_records(records)
@@ -110,7 +109,7 @@ class profile:
             use_xpu=self.use_xpu,
             profile_memory=self.profile_memory,
             with_flops=self.with_flops,
-            with_calling_stack=self.with_calling_stack,
+            with_calling_stack=self.with_calling_stack
         )
         self.function_events._build_tree()
         return False
@@ -250,17 +249,16 @@ def _parse_legacy_records(thread_records):
                 # create the function event for appending kernel
                 fe = FunctionEvent(
                     id=record.handle(),
+                    node_id=record.node_id(),
                     name=_rewrite_name(name=record.name(), with_wildcard=True),
                     thread=record.thread_id(),
                     start_us=0,
                     end_us=0,
                     stack=[],
-                    node_id=record.node_id(),
-                    input_shapes=record.shapes(),
                     cstack=tuple(calling_stack),
+                    input_shapes=record.shapes(),
                     device_type=DeviceType.CPU,
-                    is_legacy=True,
-                )
+                    is_legacy=True)
                 function_stack.append(fe)
                 range_starts[record_key] = (record, fe)
                 cpu_memory_allocs[record_key] = 0
@@ -272,8 +270,7 @@ def _parse_legacy_records(thread_records):
                 ), f"""Expected record with key {record_key} to exist in range_starts.
                     This means that the pop event did not have a corresponding push."""
 
-                start, fe = range_starts[record_key]
-
+                start, cached_fe = range_starts[record_key]
                 calling_id = calling_stack.pop()
 
                 cpu_memory_usage = cpu_memory_allocs[record_key]
@@ -283,27 +280,40 @@ def _parse_legacy_records(thread_records):
                 is_remote_event = record.is_remote()
                 start_flops = start.flops()
 
-                fe.time_range = Interval(start_record.cpu_elapsed_us(start), start_record.cpu_elapsed_us(record))
-                fe.cpu_memory_usage = cpu_memory_usage
-                fe.cuda_memory_usage = cuda_memory_usage
-                fe.xpu_memory_usage = xpu_memory_usage
-                fe.is_async = is_async
-                fe.is_remote = is_remote_event
-                fe.fwd_thread = start.fwd_thread_id()
-                fe.stack = [entry for entry in start.stack() if _filter_stack_entry(entry)]
-                fe.scope = start.scope()
-                fe.sequence_nr = start.sequence_nr()
-                fe.trace_name = _rewrite_name(name=start.name(), with_wildcard=False)
-                fe.fwd_thread = start.fwd_thread_id()
-                fe.flops = start_flops
+                fe = FunctionEvent(
+                    id=record.handle(),
+                    node_id=record.node_id(),
+                    name=_rewrite_name(name=start.name(), with_wildcard=True),
+                    trace_name=_rewrite_name(name=start.name(), with_wildcard=False),
+                    thread=start.thread_id(),
+                    start_us=start_record.cpu_elapsed_us(start),
+                    end_us=start_record.cpu_elapsed_us(record),
+                    fwd_thread=start.fwd_thread_id(),
+                    input_shapes=start.shapes(),
+                    stack=[
+                        entry for entry in start.stack() if _filter_stack_entry(entry)
+                    ],
+                    cstack=cached_fe.cstack,
+                    scope=start.scope(),
+                    cpu_memory_usage=cpu_memory_usage,
+                    cuda_memory_usage=cuda_memory_usage,
+                    xpu_memory_usage=xpu_memory_usage,
+                    is_async=is_async,
+                    is_remote=is_remote_event,
+                    sequence_nr=start.sequence_nr(),
+                    device_type=DeviceType.CPU,
+                    is_legacy=True,
+                    flops=start_flops,
+                )
+                fe.kernels = cached_fe.kernels
 
                 # note: async events have only cpu total time
                 if not is_async and start.has_cuda():
                     duration = start.cuda_elapsed_us(record)
                     if duration > 0:
-                        fe.append_kernel(start.name(), start.device(), duration)
+                        fe.append_kernel(start.name(), start.device(), "cuda", duration)
                 functions.append(fe)
-                function_stack.remove(fe)
+                function_stack.remove(cached_fe)
                 del range_starts[record_key]
                 del cpu_memory_allocs[record_key]
                 del cuda_memory_allocs[record_key]
@@ -330,7 +340,6 @@ def _parse_legacy_records(thread_records):
                         stack=[],
                         cpu_memory_usage=record.cpu_memory_usage(),
                         cuda_memory_usage=record.cuda_memory_usage(),
-                        xpu_memory_usage=record.xpu_memory_usage(),
                         is_legacy=True,
                     )
                     functions.append(fe)
@@ -341,8 +350,7 @@ def _parse_legacy_records(thread_records):
                     if len(function_stack) > 0:
                         fe = function_stack[-1]
                         fe.append_kernel(fe.name + "(" + record.name() + ")",
-                                         record.device(),
-                                         record.xpu_elapsed_us())
+                                         0, "xpu", record.xpu_elapsed_us())
                     else:
                         # An xpu event is recorded but no parent function was recorded.
                         fe = FunctionEvent(
@@ -356,10 +364,8 @@ def _parse_legacy_records(thread_records):
                             cstack=tuple(calling_stack),
                             input_shapes=record.shapes(),
                             is_legacy=True)
-                        fe.stack = []
                         fe.append_kernel(fe.name + "(" + record.name() + ")",
-                                         record.device(),
-                                         record.xpu_elapsed_us())
+                                         0, "xpu", record.xpu_elapsed_us())
                         functions.append(fe)
             prev_record = record
 
diff --git a/torch/autograd/profiler_util.py b/torch/autograd/profiler_util.py
index 3a88f72ab4e..f2ca2d81076 100644
--- a/torch/autograd/profiler_util.py
+++ b/torch/autograd/profiler_util.py
@@ -26,16 +26,16 @@ class EventList(list):
     """A list of Events (for pretty printing)"""
 
     def __init__(self, *args, **kwargs):
-        use_cuda = kwargs.pop("use_cuda", True)
-        use_device = kwargs.pop("use_device", None)
+        use_cuda = kwargs.pop("use_cuda", False)
         use_xpu = kwargs.pop("use_xpu", False)
+        use_device = kwargs.pop("use_device", None)
         profile_memory = kwargs.pop("profile_memory", False)
         with_flops = kwargs.pop("with_flops", False)
         with_calling_stack = kwargs.pop("with_calling_stack", False)
         super().__init__(*args, **kwargs)
         self._use_cuda = use_cuda
-        self._use_device = use_device
         self._use_xpu = use_xpu
+        self._use_device = use_device
         self._profile_memory = profile_memory
         self._tree_built = False
         self._with_flops = with_flops
@@ -186,14 +186,16 @@ class EventList(list):
         Args:
             sort_by (str, optional): Attribute used to sort entries. By default
                 they are printed in the same order as they were registered.
-                Valid keys include: ``cpu_time``, ``cuda_time``, ``cpu_time_total``,
-                ``cuda_time_total``, ``cpu_memory_usage``, ``cuda_memory_usage``,
-                ``self_cpu_memory_usage``, ``self_cuda_memory_usage``, ``count``.
+                Valid keys include: ``cpu_time``, ``cuda_time``, ``xpu_time``,
+                ``cpu_time_total``, ``cuda_time_total``, ``xpu_time_total``,
+                ``cpu_memory_usage``, ``cuda_memory_usage``, ``xpu_memory_usage``,
+                ``self_cpu_memory_usage``, ``self_cuda_memory_usage``,
+                ``self_xpu_memory_usage``, ``count``.
             top_level_events_only(bool, optional): Boolean flag to determine the
                 selection of events to display. If true, the profiler will only
                 display events at top level like top-level invocation of python
                 `lstm`, python `add` or other functions, nested events like low-level
-                cpu/cuda ops events are omitted for profiler result readability.
+                cpu/cuda/xpu ops events are omitted for profiler result readability.
 
         Returns:
             A string containing the table.
@@ -225,10 +227,10 @@ class EventList(list):
 
         if self._use_cuda:
              device_name = "cuda"
-        elif self._use_device:
-             device_name = self._use_device
         elif self._use_xpu:
              device_name = "xpu"
+        elif self._use_device:
+             device_name = self._use_device
         with open(path, "w") as f:
             chrome_events = []
             next_id = 0
@@ -338,8 +340,8 @@ class EventList(list):
         avg_list = EventList(
             stats.values(),
             use_cuda=self._use_cuda,
-            use_device=self._use_device,
             use_xpu=self._use_xpu,
+            use_device=self._use_device,
             profile_memory=self._profile_memory,
             with_flops=self._with_flops,
             with_calling_stack=self._with_calling_stack,
@@ -410,16 +412,16 @@ class FormattedTimesMixin:
 
     cpu_time_str = _attr_formatter("cpu_time")
     cuda_time_str = _attr_formatter("cuda_time")
-    privateuse1_time_str = _attr_formatter("privateuse1_time")
     xpu_time_str = _attr_formatter("xpu_time")
+    privateuse1_time_str = _attr_formatter("privateuse1_time")
     cpu_time_total_str = _attr_formatter("cpu_time_total")
     cuda_time_total_str = _attr_formatter("cuda_time_total")
-    privateuse1_time_total_str = _attr_formatter("privateuse1_time_total")
     xpu_time_total_str = _attr_formatter("xpu_time_total")
+    privateuse1_time_total_str = _attr_formatter("privateuse1_time_total")
     self_cpu_time_total_str = _attr_formatter("self_cpu_time_total")
     self_cuda_time_total_str = _attr_formatter("self_cuda_time_total")
-    self_privateuse1_time_total_str = _attr_formatter("self_privateuse1_time_total")
     self_xpu_time_total_str = _attr_formatter("self_xpu_time_total")
+    self_privateuse1_time_total_str = _attr_formatter("self_privateuse1_time_total")
 
     @property
     def cpu_time(self):
@@ -429,14 +431,13 @@ class FormattedTimesMixin:
     def cuda_time(self):
         return 0.0 if self.count == 0 else 1.0 * self.cuda_time_total / self.count  # type: ignore[attr-defined]
 
-    @property
-    def privateuse1_time(self):
-        return 0.0 if self.count == 0 else 1.0 * self.privateuse1_time_total / self.count  # type: ignore[attr-defined]
-
     @property
     def xpu_time(self):
         return 0.0 if self.count == 0 else 1.0 * self.xpu_time_total / self.count  # type: ignore[attr-defined]
 
+    @property
+    def privateuse1_time(self):
+        return 0.0 if self.count == 0 else 1.0 * self.privateuse1_time_total / self.count  # type: ignore[attr-defined]
 
 class Interval:
     def __init__(self, start, end):
@@ -447,7 +448,7 @@ class Interval:
         return self.end - self.start
 
 
-Kernel = namedtuple("Kernel", ["name", "device", "duration"])
+Kernel = namedtuple("Kernel", ["name", "device_id", "device_name", "duration"])
 
 
 class FunctionEvent(FormattedTimesMixin):
@@ -468,8 +469,8 @@ class FunctionEvent(FormattedTimesMixin):
         use_device=None,
         cpu_memory_usage=0,
         cuda_memory_usage=0,
-        privateuse1_memory_usage=0,
         xpu_memory_usage=0,
+        privateuse1_memory_usage=0,
         is_async=False,
         is_remote=False,
         sequence_nr=-1,
@@ -500,8 +501,8 @@ class FunctionEvent(FormattedTimesMixin):
         self.use_device: Optional[str] = use_device
         self.cpu_memory_usage: int = cpu_memory_usage
         self.cuda_memory_usage: int = cuda_memory_usage
-        self.privateuse1_memory_usage: int = privateuse1_memory_usage
         self.xpu_memory_usage: int = xpu_memory_usage
+        self.privateuse1_memory_usage: int = privateuse1_memory_usage
         self.is_async: bool = is_async
         self.is_remote: bool = is_remote
         self.sequence_nr: int = sequence_nr
@@ -510,9 +511,9 @@ class FunctionEvent(FormattedTimesMixin):
         self.is_legacy: bool = is_legacy
         self.flops: Optional[int] = flops
 
-    def append_kernel(self, name, device: torch.device, duration):
+    def append_kernel(self, name, device_name, device_id, duration):
         assert self.device_type == DeviceType.CPU
-        self.kernels.append(Kernel(name, device, duration))
+        self.kernels.append(Kernel(name, device_name, device_id, duration))
 
     def append_cpu_child(self, child):
         """Append a CPU child of type FunctionEvent.
@@ -556,18 +557,20 @@ class FunctionEvent(FormattedTimesMixin):
         )
 
     @property
-    def self_privateuse1_memory_usage(self):
+    def self_xpu_memory_usage(self):
         if self.is_async or self.device_type != DeviceType.CPU:
             return 0
-        return self.privateuse1_memory_usage - sum(
-            [child.privateuse1_memory_usage for child in self.cpu_children]
+        return self.xpu_memory_usage - sum(
+            [child.xpu_memory_usage for child in self.cpu_children]
         )
 
     @property
-    def self_xpu_memory_usage(self):
-        if self.is_async:
+    def self_privateuse1_memory_usage(self):
+        if self.is_async or self.device_type != DeviceType.CPU:
             return 0
-        return self.xpu_memory_usage - sum([child.xpu_memory_usage for child in self.cpu_children])
+        return self.privateuse1_memory_usage - sum(
+            [child.privateuse1_memory_usage for child in self.cpu_children]
+        )
 
     @property
     def self_cpu_time_total(self):
@@ -584,11 +587,11 @@ class FunctionEvent(FormattedTimesMixin):
         if self.device_type == DeviceType.CPU:
             if not self.is_legacy:
                 # account for the kernels in the children ops
-                return (sum(kinfo.duration for kinfo in self.kernels if kinfo.device.type == DeviceType.CUDA) +
-                        sum(ch.cuda_time_total for ch in self.cpu_children))
+                return sum(kinfo.duration for kinfo in self.kernels if kinfo.device_name == 'cuda') \
+                     + sum(ch.cuda_time_total for ch in self.cpu_children)
             else:
                 # each legacy cpu events has a single (fake) kernel
-                return sum(kinfo.duration for kinfo in self.kernels if kinfo.device.type == DeviceType.CUDA)
+                return sum(kinfo.duration for kinfo in self.kernels if kinfo.device_name == 'cuda')
         elif self.device_type == DeviceType.CUDA:
             return self.time_range.elapsed_us()
         else:
@@ -614,6 +617,35 @@ class FunctionEvent(FormattedTimesMixin):
         else:
             return 0
 
+    @property
+    def xpu_time_total(self):
+        if self.is_async or self.use_device:
+            return 0
+        if self.device_type == DeviceType.CPU:
+            if not self.is_legacy:
+                return sum(kinfo.duration for kinfo in self.kernels if kinfo.device_name == "xpu") \
+                     + sum(ch.xpu_time_total for ch in self.cpu_children)
+            else:
+                return sum(kinfo.duration for kinfo in self.kernels if kinfo.device_name == "xpu") \
+                     + sum(ch.xpu_time_total for ch in self.cpu_children)
+        elif self.device_type == DeviceType.XPU:
+            return self.time_range.elapsed_us()
+        else:
+            return 0
+
+    @property
+    def self_xpu_time_total(self):
+        if self.is_async or self.use_device:
+            return 0
+        if self.device_type == DeviceType.CPU:
+            return self.xpu_time_total - sum(
+                [child.xpu_time_total for child in self.cpu_children]
+            )
+        elif self.device_type == DeviceType.XPU:
+            return self.xpu_time_total
+        else:
+            return 0
+
     @property
     def self_privateuse1_time_total(self):
         if self.is_async or not self.use_device:
@@ -622,9 +654,10 @@ class FunctionEvent(FormattedTimesMixin):
             return self.privateuse1_time_total - sum(
                 [child.privateuse1_time_total for child in self.cpu_children]
             )
-        else:
-            assert self.device_type == DeviceType.CUDA
+        elif self.device_type == DeviceType.PrivateUse1:
             return self.privateuse1_time_total
+        else:
+            return 0
 
     @property
     def privateuse1_time_total(self):
@@ -633,53 +666,33 @@ class FunctionEvent(FormattedTimesMixin):
         if self.device_type == DeviceType.CPU:
             if not self.is_legacy:
                 # account for the kernels in the children ops
-                return sum(kinfo.duration for kinfo in self.kernels) + sum(
-                    ch.privateuse1_time_total for ch in self.cpu_children
-                )
+                return sum(kinfo.duration for kinfo in self.kernels if kinfo.device_name == "privateuse1") \
+                     + sum(ch.privateuse1_time_total for ch in self.cpu_children)
             else:
                 # each legacy cpu events has a single (fake) kernel
                 return sum(kinfo.duration for kinfo in self.kernels)
-        else:
-            assert self.device_type == DeviceType.PrivateUse1
-            return self.time_range.elapsed_us()
-
-    @property
-    def xpu_time_total(self):
-        if self.is_async:
-            return 0
-        if self.device_type == DeviceType.CPU:
-            # account for the kernels in the children ops
-            return (sum(kinfo.duration for kinfo in self.kernels if kinfo.device.type == "xpu") +
-                    sum(ch.xpu_time_total for ch in self.cpu_children))
-        elif self.device_type == DeviceType.XPU:
+        elif self.device_type == DeviceType.PrivateUse1:
             return self.time_range.elapsed_us()
         else:
             return 0
 
-    @property
-    def self_xpu_time_total(self):
-        if self.is_async:
-            return 0
-        else:
-            return sum(kinfo.duration for kinfo in self.kernels if kinfo.device.type == "xpu")
-
     @property
     def key(self):
         return self.name
 
     def __repr__(self):
-        if self.device_type == "cuda":
-            device_name = "cuda"
-            device_time = self.cuda_time_str
-            device_memory_usage = self.cuda_memory_usage
+        if self.device_type == "xpu":
+            device_name = "xpu"
+            device_time = self.xpu_time_str
+            device_memory_usage = self.xpu_memory_usage
         elif self.use_device:
             device_name = self.use_device
             device_time = self.privateuse1_time_str
             device_memory_usage = self.privateuse1_memory_usage
-        elif self.device_type == "xpu":
-            device_name = "xpu"
-            device_time = self.xpu_time_str
-            device_memory_usage = self.xpu_memory_usage
+        else:
+            device_name = "cuda"
+            device_time = self.cuda_time_str
+            device_memory_usage = self.cuda_memory_usage
         return (
             "<FunctionEvent id={} name={} device_type={} node_id={} cpu_time={} start_us={} end_us={} "
             "cpu_children={} {}_time={} name={} thread={} input_shapes={} cstack={} "
@@ -721,24 +734,24 @@ class FunctionEventAvg(FormattedTimesMixin):
         self.use_device: Optional[str] = None
         self.cpu_time_total: int = 0
         self.cuda_time_total: int = 0
-        self.privateuse1_time_total: int = 0
         self.xpu_time_total: int = 0
+        self.privateuse1_time_total: int = 0
         self.self_cpu_time_total: int = 0
         self.self_cuda_time_total: int = 0
-        self.self_privateuse1_time_total: int = 0
         self.self_xpu_time_total: int = 0
+        self.self_privateuse1_time_total: int = 0
         self.input_shapes: Optional[List[List[int]]] = None
         self.stack: Optional[List] = None
         self.cstack: Optional[List] = None
         self.scope: Optional[int] = None
         self.cpu_memory_usage: int = 0
         self.cuda_memory_usage: int = 0
-        self.privateuse1_memory_usage: int = 0
         self.xpu_memory_usage: int = 0
+        self.privateuse1_memory_usage: int = 0
         self.self_cpu_memory_usage: int = 0
         self.self_cuda_memory_usage: int = 0
-        self.self_privateuse1_memory_usage: int = 0
         self.self_xpu_memory_usage: int = 0
+        self.self_privateuse1_memory_usage: int = 0
         self.cpu_children: Optional[List[FunctionEvent]] = None
         self.cpu_parent: Optional[FunctionEvent] = None
         self.device_type: DeviceType = DeviceType.CPU
@@ -768,20 +781,20 @@ class FunctionEventAvg(FormattedTimesMixin):
         assert other.key == self.key
         self.cpu_time_total += other.cpu_time_total
         self.cuda_time_total += other.cuda_time_total
-        self.privateuse1_time_total += other.privateuse1_time_total
         self.xpu_time_total += other.xpu_time_total
+        self.privateuse1_time_total += other.privateuse1_time_total
         self.self_cpu_time_total += other.self_cpu_time_total
         self.self_cuda_time_total += other.self_cuda_time_total
-        self.self_privateuse1_time_total += other.self_privateuse1_time_total
         self.self_xpu_time_total += other.self_xpu_time_total
+        self.self_privateuse1_time_total += other.self_privateuse1_time_total
         self.cpu_memory_usage += other.cpu_memory_usage
         self.cuda_memory_usage += other.cuda_memory_usage
-        self.privateuse1_memory_usage += other.privateuse1_memory_usage
         self.xpu_memory_usage += other.xpu_memory_usage
+        self.privateuse1_memory_usage += other.privateuse1_memory_usage
         self.self_cpu_memory_usage += other.self_cpu_memory_usage
         self.self_cuda_memory_usage += other.self_cuda_memory_usage
-        self.self_privateuse1_memory_usage += other.self_privateuse1_memory_usage
         self.self_xpu_memory_usage += other.self_xpu_memory_usage
+        self.self_privateuse1_memory_usage += other.self_privateuse1_memory_usage
         self.count += other.count
         if self.flops is None:
             self.flops = other.flops
@@ -793,21 +806,21 @@ class FunctionEventAvg(FormattedTimesMixin):
         return self.add(other)
 
     def __repr__(self):
-        if self.device_type == "cuda":
-            device_name = "cuda"
-            self_device_time = self.self_cuda_time_total_str
-            device_time = self.cuda_time_str
-            device_memory = self.cuda_memory_usage
+        if self.device_type == "xpu":
+            device_name = "xpu"
+            self_device_time = self.self_xpu_time_total_str
+            device_time = self.xpu_time_str
+            device_memory = self.xpu_memory_usage
         elif self.use_device:
             device_name = self.use_device
             self_device_time = self.self_privateuse1_time_total_str
             device_time = self.privateuse1_time_str
             device_memory = self.privateuse1_memory_usage
-        elif self.device_type == "xpu":
-            device_name = "xpu"
-            self_device_time = self.self_xpu_time_total_str
-            device_time = self.xpu_time_str
-            device_memory = self.xpu_memory_usage
+        else:
+            device_name = "cuda"
+            self_device_time = self.self_cuda_time_total_str
+            device_time = self.cuda_time_str
+            device_memory = self.cuda_memory_usage
         return (
             "<FunctionEventAvg key={} self_cpu_time={} cpu_time={} "
             " self_{}_time={} {}_time={} input_shapes={} cstack={} "
@@ -919,14 +932,14 @@ def _build_table(
 
     has_cuda_time = any(event.self_cuda_time_total > 0 for event in events)
     has_cuda_mem = any(event.self_cuda_memory_usage > 0 for event in events)
+    has_xpu_time = any(event.self_xpu_time_total > 0 for event in events)
+    has_xpu_mem = any(event.self_xpu_memory_usage > 0 for event in events)
     has_privateuse1_time = any(
         event.self_privateuse1_time_total > 0 for event in events
     )
     has_privateuse1_mem = any(
         event.self_privateuse1_memory_usage > 0 for event in events
     )
-    has_xpu_time = any([event.self_xpu_time_total > 0 for event in events])
-    has_xpu_mem = any([event.self_xpu_memory_usage > 0 for event in events])
     use_device = events[0].use_device
     if not use_device and (has_privateuse1_mem or has_privateuse1_time):
         raise RuntimeError(
@@ -942,8 +955,8 @@ def _build_table(
         events = EventList(
             sorted(events, key=lambda evt: getattr(evt, sort_by), reverse=True),
             use_cuda=has_cuda_time,
-            use_device=use_device,
             use_xpu=has_xpu_time,
+            use_device=use_device,
             profile_memory=profile_memory,
             with_flops=with_flops,
             with_calling_stack=with_calling_stack,
@@ -994,6 +1007,15 @@ def _build_table(
                 "CUDA time avg",
             ]
         )
+    if has_xpu_time:
+        headers.extend(
+            [
+                "Self XPU",
+                "Self XPU %",
+                "XPU total",
+                "XPU time avg",
+            ]
+        )
     if has_privateuse1_time:
         privateuse1 = use_device.upper()
         headers.extend(
@@ -1004,15 +1026,6 @@ def _build_table(
                 f"{privateuse1} time avg",
             ]
         )
-    if has_xpu_time:
-        headers.extend(
-            [
-                "Self XPU",
-                "Self XPU %",
-                "XPU total",
-                "XPU time avg",
-            ]
-        )
     if profile_memory:
         headers.extend(
             [
@@ -1027,19 +1040,19 @@ def _build_table(
                     "Self CUDA Mem",
                 ]
             )
-        if has_privateuse1_mem:
-            privateuse1 = use_device.upper()
+        if has_xpu_mem:
             headers.extend(
                 [
-                    f"{privateuse1} Mem",
-                    f"Self {privateuse1} Mem",
+                    "XPU Mem",
+                    "Self XPU Mem",
                 ]
             )
-        if has_xpu_mem:
+        if has_privateuse1_mem:
+            privateuse1 = use_device.upper()
             headers.extend(
                 [
-                    "XPU Mem",
-                    "Self XPU Mem",
+                    f"{privateuse1} Mem",
+                    f"Self {privateuse1} Mem",
                 ]
             )
     headers.append("# of Calls")
@@ -1119,8 +1132,8 @@ def _build_table(
 
     sum_self_cpu_time_total = sum([event.self_cpu_time_total for event in events])
     sum_self_cuda_time_total = 0
-    sum_self_privateuse1_time_total = 0
     sum_self_xpu_time_total = 0
+    sum_self_privateuse1_time_total = 0
     for evt in events:
         if evt.device_type == DeviceType.CPU:
             # in legacy profiler, kernel info is stored in cpu events
@@ -1133,6 +1146,8 @@ def _build_table(
         elif evt.device_type == DeviceType.CUDA:
             # in kineto profiler, there're events with the correct device type (e.g. CUDA)
             sum_self_cuda_time_total += evt.self_cuda_time_total
+        elif evt.device_type == DeviceType.XPU:
+            sum_self_xpu_time_total += evt.self_xpu_time_total
         elif evt.device_type == DeviceType.PrivateUse1:
             sum_self_privateuse1_time_total += evt.self_privateuse1_time_total
         elif evt.device_type == DeviceType.XPU:
@@ -1194,28 +1209,28 @@ def _build_table(
                     evt.cuda_time_str,  # Cuda time avg
                 ]
             )
-        if has_privateuse1_time:
+        if has_xpu_time:
             row_values.extend(
                 [
-                    evt.self_privateuse1_time_total_str,
-                    # PrivateUse1 time total %
+                    evt.self_xpu_time_total_str,
+                    # XPU time total %
                     _format_time_share(
-                        evt.self_privateuse1_time_total, sum_self_privateuse1_time_total
+                        evt.self_xpu_time_total, sum_self_xpu_time_total
                     ),
-                    evt.privateuse1_time_total_str,
-                    evt.privateuse1_time_str,  # PrivateUse1 time avg
+                    evt.xpu_time_total_str,
+                    evt.xpu_time_str,  # XPU time avg
                 ]
             )
-        if has_xpu_time:
+        if has_privateuse1_time:
             row_values.extend(
                 [
-                    evt.self_xpu_time_total_str,
-                    # SYCL time total %
+                    evt.self_privateuse1_time_total_str,
+                    # PrivateUse1 time total %
                     _format_time_share(
-                        evt.self_xpu_time_total, sum_self_xpu_time_total
+                        evt.self_privateuse1_time_total, sum_self_privateuse1_time_total
                     ),
-                    evt.xpu_time_total_str,
-                    evt.xpu_time_str,   # SYCL time avg
+                    evt.privateuse1_time_total_str,
+                    evt.privateuse1_time_str,  # PrivateUse1 time avg
                 ]
             )
         if profile_memory:
@@ -1236,6 +1251,15 @@ def _build_table(
                         _format_memory(evt.self_cuda_memory_usage),
                     ]
                 )
+            if has_xpu_mem:
+                row_values.extend(
+                    [
+                        # XPU Mem Total
+                        _format_memory(evt.xpu_memory_usage),
+                        # Self XPU Mem Total
+                        _format_memory(evt.self_xpu_memory_usage),
+                    ]
+                )
             if has_privateuse1_mem:
                 row_values.extend(
                     [
@@ -1245,15 +1269,6 @@ def _build_table(
                         _format_memory(evt.self_privateuse1_memory_usage),
                     ]
                 )
-            if has_xpu_mem:
-                row_values.extend(
-                    [
-                        # SYCL Mem Total
-                        _format_memory(evt.xpu_memory_usage),
-                        # Self SYCL Mem Total
-                        _format_memory(evt.self_xpu_memory_usage),
-                    ]
-                )
         row_values.append(
             evt.count,  # Number of calls
         )
@@ -1295,10 +1310,10 @@ def _build_table(
     append(f"Self CPU time total: {_format_time(sum_self_cpu_time_total)}")
     if has_cuda_time:
         append(f"Self CUDA time total: {_format_time(sum_self_cuda_time_total)}")
+    if has_xpu_time:
+        append(f"Self XPU time total: {_format_time(sum_self_xpu_time_total)}")
     if has_privateuse1_time:
         append(
             f"Self {use_device.upper()} time total: {_format_time(sum_self_privateuse1_time_total)}"
         )
-    if has_xpu_time:
-        append(f"Self XPU time total: {_format_time(sum_self_xpu_time_total)}")
     return "".join(result)
diff --git a/torch/csrc/profiler/kineto_shim.cpp b/torch/csrc/profiler/kineto_shim.cpp
index 59e721eec41..3138e56065f 100644
--- a/torch/csrc/profiler/kineto_shim.cpp
+++ b/torch/csrc/profiler/kineto_shim.cpp
@@ -1,6 +1,8 @@
 #include <torch/csrc/profiler/collection.h>
 #include <torch/csrc/profiler/kineto_shim.h>
 
+#include <ATen/Context.h>
+
 #include <type_traits>
 
 #ifdef USE_KINETO
@@ -320,6 +322,15 @@ void logInvariantViolation(
 namespace autograd {
 namespace profiler {
 c10::DeviceType deviceTypeFromActivity(libkineto::ActivityType activity_type) {
+  // gpu_device is for returning currently used device type for Activity
+  // such as concurrent kernel or gpu memory operations
+  // It should be one of registered non-CPU device, so we can init it with
+  // CPU as a invalid value for checking.
+  c10::DeviceType gpu_device = c10::DeviceType::CPU;
+  if (at::hasCUDA() || at::hasMTIA())
+    gpu_device = c10::DeviceType::CUDA;
+  else if(at::hasXPU())
+    gpu_device = c10::DeviceType::XPU;
   // fallthrough
   switch (activity_type) {
     case libkineto::ActivityType::GPU_MEMCPY:
@@ -330,13 +341,17 @@ c10::DeviceType deviceTypeFromActivity(libkineto::ActivityType activity_type) {
     case libkineto::ActivityType::CUDA_PROFILER_RANGE:
     // TODO: T151322015
     case libkineto::ActivityType::MTIA_CCP_EVENTS:
-      return c10::DeviceType::CUDA;
+      TORCH_CHECK(gpu_device != c10::DeviceType::CPU,
+                  "Kineto GPU Activity Type enabled, but no available gpu device was found."
+                  "Kineto allowed GPU device must be one of {CUDA, MTIA or XPU}");
+      return gpu_device;
     case libkineto::ActivityType::CPU_OP:
     case libkineto::ActivityType::USER_ANNOTATION:
     case libkineto::ActivityType::EXTERNAL_CORRELATION:
     case libkineto::ActivityType::CUDA_RUNTIME:
     case libkineto::ActivityType::CPU_INSTANT_EVENT:
     case libkineto::ActivityType::GLOW_RUNTIME:
+    case libkineto::ActivityType::XPU_RUNTIME:
     case libkineto::ActivityType::MTIA_RUNTIME:
     case libkineto::ActivityType::PYTHON_FUNCTION:
     case libkineto::ActivityType::CUDA_DRIVER:
diff --git a/torch/profiler/profiler.py b/torch/profiler/profiler.py
index fce2e988471..16950ba5040 100644
--- a/torch/profiler/profiler.py
+++ b/torch/profiler/profiler.py
@@ -51,7 +51,8 @@ class _KinetoProfile:
 
     Args:
         activities (iterable): list of activity groups (CPU, CUDA) to use in profiling, supported values:
-            ``torch.profiler.ProfilerActivity.CPU``, ``torch.profiler.ProfilerActivity.CUDA``.
+            ``torch.profiler.ProfilerActivity.CPU``, ``torch.profiler.ProfilerActivity.CUDA``,
+            ``torch.profiler.ProfilerActivity.XPU``.
             Default value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA.
         record_shapes (bool): save information about operator's input shapes.
         profile_memory (bool): track tensor memory allocation/deallocation.
@@ -112,6 +113,7 @@ class _KinetoProfile:
     def prepare_trace(self):
         self.profiler = prof.profile(
             use_cuda=(ProfilerActivity.CUDA in self.activities),
+            use_xpu=(ProfilerActivity.XPU in self.activities),
             use_cpu=(ProfilerActivity.CPU in self.activities),
             use_mtia=(ProfilerActivity.MTIA in self.activities),
             use_device=None,
@@ -261,11 +263,14 @@ class _KinetoProfile:
         Output: File written as JSON or gzipped JSON
         """
         # Default to device 0, if unset. Fallback on cpu.
-        if device is None and self.use_device and self.use_device != "cuda":
+        if device is None and self.use_device and self.use_device not in ["cuda", "xpu"]:
             device = self.use_device + ":0"
 
         if device is None:
-            device = "cuda:0" if torch.cuda.is_available() else "cpu"
+            if hasattr(torch, 'xpu') and torch.xpu.is_available():  # type: ignore[attr-defined]
+                device = "xpu:0"
+            else:
+                device = "cuda:0" if torch.cuda.is_available() else "cpu"
 
         # Construct the memory timeline plot data
         self.mem_tl = MemoryProfileTimeline(self._memory_profile())
@@ -384,7 +389,8 @@ class profile(_KinetoProfile):
 
     Args:
         activities (iterable): list of activity groups (CPU, CUDA) to use in profiling, supported values:
-            ``torch.profiler.ProfilerActivity.CPU``, ``torch.profiler.ProfilerActivity.CUDA``.
+            ``torch.profiler.ProfilerActivity.CPU``, ``torch.profiler.ProfilerActivity.CUDA``,
+            ``torch.profiler.ProfilerActivity.XPU``.
             Default value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA.
         schedule (Callable): callable that takes step (int) as a single parameter and returns
             ``ProfilerAction`` value that specifies the profiler action to perform at each step.
-- 
2.34.1

