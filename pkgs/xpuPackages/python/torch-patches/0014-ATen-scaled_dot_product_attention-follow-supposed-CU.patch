From a2a7717a2d25ab71869a33600a28b4728fb8b70d Mon Sep 17 00:00:00 2001
From: majing <jing1.ma@intel.com>
Date: Mon, 23 Oct 2023 11:33:05 +0800
Subject: [PATCH 14/17] ATen scaled_dot_product_attention: follow supposed
 (CUDA/CPU) operators registration strategy (#177)

* Enable XPU backend implementation for scaled_dot_product_attention

Signed-off-by: majing <Jing1.Ma@intel.com>

* fixed format

Signed-off-by: majing <Jing1.Ma@intel.com>

---------

Signed-off-by: majing <Jing1.Ma@intel.com>
---
 aten/src/ATen/native/transformers/attention.cpp | 17 +++++++++++++++++
 1 file changed, 17 insertions(+)

diff --git a/aten/src/ATen/native/transformers/attention.cpp b/aten/src/ATen/native/transformers/attention.cpp
index 81498f97ffa..131999d6040 100644
--- a/aten/src/ATen/native/transformers/attention.cpp
+++ b/aten/src/ATen/native/transformers/attention.cpp
@@ -526,6 +526,21 @@ int64_t _fused_sdp_choice_meta(
         scale);
     return choice_int;
   }
+
+  bool has_xpu = query_key_set.has(c10::DispatchKey::XPU);
+  if (has_xpu) {
+    auto choice_int = at::_ops::_fused_sdp_choice::redispatch(
+        c10::DispatchKeySet(c10::DispatchKey::XPU),
+        query_,
+        key,
+        value,
+        attn_mask_,
+        dropout_p,
+        is_causal,
+        scale);
+    return choice_int;
+  }
+
   return static_cast<int64_t>(sdp::SDPBackend::math);
 }
 namespace {
@@ -674,6 +689,8 @@ Tensor scaled_dot_product_attention(
       || query_.device().type() == DeviceType::CPU){
     choice_int = _fused_sdp_choice_stub(query_.device().type(),
       query_, key, value, attn_mask_, dropout_p, is_causal, scale);
+  } else if (query_.device().type() == DeviceType::XPU) {
+    choice_int = _fused_sdp_choice_meta(query_, key, value, attn_mask_, dropout_p, is_causal, scale);
   }
   sdp::SDPBackend backend = static_cast<sdp::SDPBackend>(choice_int);
   c10::optional<Tensor> attn_mask = convert_boolean_attn_mask(attn_mask_, query_.dtype());
-- 
2.34.1

