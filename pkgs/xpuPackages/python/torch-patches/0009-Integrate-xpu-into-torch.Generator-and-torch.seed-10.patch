From 6c5a85526546347ae39d66fca5beb31af09d1c16 Mon Sep 17 00:00:00 2001
From: "Yu, Guangye" <106960996+guangyey@users.noreply.github.com>
Date: Thu, 28 Sep 2023 10:07:40 +0800
Subject: [PATCH 09/17] Integrate xpu into torch.Generator and torch.seed
 (#109866) (#110220) (#110376) (#169)

Integrate torch.xpu.Generator into torch.Generator
Integrate torch.xpu.seed into torch.seed
Pull Request resolved: https://github.com/pytorch/pytorch/pull/109866
Approved by: https://github.com/ezyang

fix a missing keyword virtual
Pull Request resolved: https://github.com/pytorch/pytorch/pull/110220
Approved by: https://github.com/ezyang

use torch.xpu.manual_seed_all in torch.seed
Pull Request resolved: https://github.com/pytorch/pytorch/pull/110376
Approved by: https://github.com/ezyang

(cherry picked from commit 633bd0765ee789a7ec4d843d5b76c655718a54d7)
(cherry picked from commit c4ad64c02c62d4170cf19622e9b0103fe4564ce8)
(cherry picked from commit 0063a81910813c8ce916194aeda0899c8be66fd7)

Co-authored-by: Lei, Zhenyuan <zhenyuan.lei@intel.com>
---
 aten/src/ATen/Context.h                  | 21 ++++++++++++++++++---
 aten/src/ATen/detail/XPUHooksInterface.h | 16 +++++++++++++++-
 torch/csrc/Generator.cpp                 |  5 ++++-
 torch/random.py                          |  6 ++++++
 4 files changed, 43 insertions(+), 5 deletions(-)

diff --git a/aten/src/ATen/Context.h b/aten/src/ATen/Context.h
index 285c6f7f1f3..2ce33b77c4d 100644
--- a/aten/src/ATen/Context.h
+++ b/aten/src/ATen/Context.h
@@ -44,6 +44,8 @@ class TORCH_API Context {
       return at::detail::getCUDAHooks().getDefaultCUDAGenerator(device.index());
     } else if (device_type == at::kMPS) {
       return at::detail::getMPSHooks().getDefaultMPSGenerator();
+    } else if (device_type == at::kXPU) {
+      return at::detail::getXPUHooks().getDefaultXPUGenerator(device.index());
     } else if (device_type == at::kPrivateUse1) {
       return at::GetPrivateUse1HooksInterface()->getDefaultGenerator(
           device.index());
@@ -445,9 +447,9 @@ static inline void manual_seed(uint64_t seed) {
   }
   // NB: Sometimes we build with CUDA, but we don't have any GPUs
   // available. In that case, we must not seed CUDA; it will fail!
-  const auto num_gpus = detail::getCUDAHooks().getNumGPUs();
-  if (hasCUDA() && num_gpus > 0) {
-    for (const auto i : c10::irange(num_gpus)) {
+  const auto cuda_num_gpus = detail::getCUDAHooks().getNumGPUs();
+  if (hasCUDA() && cuda_num_gpus > 0) {
+    for (const auto i : c10::irange(cuda_num_gpus)) {
       auto cuda_gen = globalContext().defaultGenerator(
           Device(at::kCUDA, static_cast<c10::DeviceIndex>(i)));
       {
@@ -458,6 +460,19 @@ static inline void manual_seed(uint64_t seed) {
     }
   }
 
+  const auto xpu_num_gpus = detail::getXPUHooks().getNumGPUs();
+  if (hasXPU() && xpu_num_gpus > 0) {
+    for (const auto i : c10::irange(xpu_num_gpus)) {
+      auto xpu_gen = globalContext().defaultGenerator(
+          Device(at::kXPU, static_cast<c10::DeviceIndex>(i)));
+      {
+        // See Note [Acquire lock when using random generators]
+        std::lock_guard<std::mutex> lock(xpu_gen.mutex());
+        xpu_gen.set_current_seed(seed);
+      }
+    }
+  }
+
   if (hasMPS()) {
     auto mps_gen = globalContext().defaultGenerator(c10::DeviceType::MPS);
     // See Note [Acquire lock when using random generators]
diff --git a/aten/src/ATen/detail/XPUHooksInterface.h b/aten/src/ATen/detail/XPUHooksInterface.h
index 34f3a10fed6..bc40baebf1c 100644
--- a/aten/src/ATen/detail/XPUHooksInterface.h
+++ b/aten/src/ATen/detail/XPUHooksInterface.h
@@ -2,7 +2,7 @@
 
 #include <c10/core/Device.h>
 #include <c10/util/Exception.h>
-
+#include <ATen/core/Generator.h>
 #include <c10/util/Registry.h>
 
 #include <cstddef>
@@ -66,6 +66,20 @@ struct TORCH_API XPUHooksInterface {
         "Cannot get XPU DL device without Intel Extension for Pytorch. ",
         XPU_HELP);
   }
+
+  virtual Generator getXPUGenerator(DeviceIndex device_index = -1) const {
+    (void)device_index; // Suppress unused variable warning
+    TORCH_CHECK(false, "Cannot get XPU generator without Intel Extension for Pytorch. ", XPU_HELP);
+  }
+
+  virtual const Generator& getDefaultXPUGenerator(DeviceIndex device_index = -1) const {
+    (void)device_index; // Suppress unused variable warning
+    TORCH_CHECK(false, "Cannot get default XPU generator without Intel Extension for Pytorch. ", XPU_HELP);
+  }
+
+  virtual int getNumGPUs() const {
+    return 0;
+  }
 };
 
 struct TORCH_API XPUHooksArgs {};
diff --git a/torch/csrc/Generator.cpp b/torch/csrc/Generator.cpp
index 9f86f3e837c..065564c577c 100644
--- a/torch/csrc/Generator.cpp
+++ b/torch/csrc/Generator.cpp
@@ -5,6 +5,7 @@
 #include <structmember.h>
 
 #include <ATen/core/GeneratorForPrivateuseone.h>
+#include <ATen/detail/XPUHooksInterface.h>
 #include <torch/csrc/Device.h>
 #include <torch/csrc/Exceptions.h>
 #include <torch/csrc/THP.h>
@@ -71,7 +72,9 @@ static PyObject* THPGenerator_pynew(
     self->cdata = make_generator<MPSGeneratorImpl>();
   }
 #endif
-  else if (device.type() == at::kPrivateUse1) {
+  else if (device.type() == at::kXPU) {
+    self->cdata = at::detail::getXPUHooks().getXPUGenerator(device.index());
+  } else if (device.type() == at::kPrivateUse1) {
     self->cdata = at::GetGeneratorForPrivateuse1(device.index());
   } else {
     AT_ERROR(
diff --git a/torch/random.py b/torch/random.py
index 86a9a304d98..668443a2b2d 100644
--- a/torch/random.py
+++ b/torch/random.py
@@ -43,6 +43,9 @@ def manual_seed(seed) -> torch._C.Generator:
     if not torch.mps._is_in_bad_fork():
         torch.mps.manual_seed(seed)
 
+    if hasattr(torch, 'xpu') and not torch.xpu._is_in_bad_fork():
+        torch.xpu.manual_seed_all(seed)
+
     _seed_custom_device(seed)
 
     return default_generator.manual_seed(seed)
@@ -62,6 +65,9 @@ def seed() -> int:
     if not torch.mps._is_in_bad_fork():
         torch.mps.manual_seed(seed)
 
+    if hasattr(torch, 'xpu') and not torch.xpu._is_in_bad_fork():
+        torch.xpu.manual_seed_all(seed)
+
     _seed_custom_device(seed)
 
     return seed
-- 
2.34.1

